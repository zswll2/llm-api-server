# LLM API Server

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

ä¸€ä¸ªé«˜æ€§èƒ½ã€å…¼å®¹OpenAI APIçš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å™¨ï¼Œæ”¯æŒå¤šç§å¼€æºLLMæ¨¡å‹ï¼ŒåŸºäºFastAPIå’ŒvLLMæ„å»ºã€‚

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸš€ **é«˜æ€§èƒ½æ¨ç†**ï¼šåŸºäºvLLMå®ç°é«˜ååé‡å’Œä½å»¶è¿Ÿçš„æ¨¡å‹æ¨ç†
- ğŸ”„ **OpenAIå…¼å®¹API**ï¼šå®Œå…¨å…¼å®¹OpenAI APIï¼Œå¯ç›´æ¥æ›¿ä»£OpenAIæœåŠ¡
- ğŸ”Œ **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒå„ç§å¼€æºLLMï¼Œå¦‚Qwenã€DeepSeekã€Llamaç­‰
- ğŸ”’ **APIå¯†é’¥è®¤è¯**ï¼šå†…ç½®APIå¯†é’¥ç®¡ç†å’Œæƒé™æ§åˆ¶
- ğŸŒŠ **æµå¼è¾“å‡º**ï¼šæ”¯æŒæµå¼å“åº”ï¼Œå‡å°‘é¦–å­—å»¶è¿Ÿ
- ğŸ”„ **åŠ¨æ€æ¨¡å‹åŠ è½½**ï¼šæŒ‰éœ€åŠ è½½å’Œå¸è½½æ¨¡å‹ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨
- âš™ï¸ **çµæ´»é…ç½®**ï¼šé€šè¿‡YAMLé…ç½®æ–‡ä»¶è¿›è¡Œå…¨é¢å®šåˆ¶

## å®‰è£…æŒ‡å—

### ç³»ç»Ÿè¦æ±‚

- Python 3.10+
- CUDA 11.8+
- è‡³å°‘16GB GPUå†…å­˜ï¼ˆå–å†³äºæ‰€ä½¿ç”¨çš„æ¨¡å‹ï¼‰

### å®‰è£…æ­¥éª¤

1. å…‹éš†ä»“åº“

```bash
git clone https://github.com/zswll2/llm-api-server.git
cd llm-api-server
```

2. åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–è€…
venv\Scripts\activate  # Windows
```

3. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

4. é…ç½®æœåŠ¡å™¨

å¤åˆ¶ç¤ºä¾‹é…ç½®æ–‡ä»¶å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ï¼š

```bash
cp config.example.yaml config.yaml
# ä½¿ç”¨æ‚¨å–œæ¬¢çš„ç¼–è¾‘å™¨ç¼–è¾‘config.yaml
```

5. ä¸‹è½½æ¨¡å‹

ä¸‹è½½æ‚¨æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¹¶ç¡®ä¿åœ¨é…ç½®æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®æ¨¡å‹è·¯å¾„ã€‚

## é…ç½®è¯´æ˜

`config.yaml`æ–‡ä»¶åŒ…å«äº†æœåŠ¡å™¨çš„æ‰€æœ‰é…ç½®é€‰é¡¹ï¼š

```yaml
# æœåŠ¡é…ç½®
server:
  host: "0.0.0.0"  # ç›‘å¬åœ°å€
  port: 8000        # ç›‘å¬ç«¯å£
  workers: 1        # å·¥ä½œè¿›ç¨‹æ•°
  timeout: 300      # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# è®¤è¯é…ç½®
auth:
  enabled: true     # æ˜¯å¦å¯ç”¨è®¤è¯
  secret_key: "your-secret-key"  # JWTå¯†é’¥
  algorithm: "HS256"            # JWTç®—æ³•
  api_keys:         # é¢„è®¾APIå¯†é’¥åˆ—è¡¨
    - key: "sk-myapikey123456789"
      name: "default"
      permissions: ["all"]
    - key: "sk-restricted987654321"
      name: "restricted"
      permissions: ["chat"]

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"     # æ—¥å¿—çº§åˆ«
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
  retention: "7 days"
  rotation: "1 day"
  path: "logs"       # æ—¥å¿—ç›®å½•
  console: true      # æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°

# æ¨¡å‹é…ç½®
models:
  default: "DeepSeek-R1-0528-Qwen3-8B"  # é»˜è®¤æ¨¡å‹
  preload: ["DeepSeek-R1-0528-Qwen3-8B"]  # é¢„åŠ è½½æ¨¡å‹
  max_loaded: 2      # æœ€å¤§åŒæ—¶åŠ è½½æ¨¡å‹æ•°
  unload_timeout: 3600  # æ¨¡å‹ä¸æ´»è·ƒå¤šä¹…åå¸è½½ï¼ˆç§’ï¼‰
  models_dir: "models"  # æ¨¡å‹ç›®å½•
  
  # å¯ç”¨æ¨¡å‹åˆ—è¡¨
  available:
    DeepSeek-R1-0528-Qwen3-8B:
      path: "/path/to/model"  # æ¨¡å‹è·¯å¾„
      type: "qwen"           # æ¨¡å‹ç±»å‹
      max_tokens: 4096        # æœ€å¤§tokenæ•°
      quantization: null      # é‡åŒ–æ–¹å¼
      tensor_parallel_size: 1  # å¼ é‡å¹¶è¡Œå¤§å°
      gpu_memory_utilization: 0.8  # GPUå†…å­˜åˆ©ç”¨ç‡
      trust_remote_code: true  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 

# æ¨ç†é…ç½®
inference:
  max_total_tokens: 4096  # è¾“å…¥+è¾“å‡ºçš„æœ€å¤§tokenæ•°
  max_input_tokens: 3072  # æœ€å¤§è¾“å…¥tokenæ•°
  max_batch_size: 32      # æœ€å¤§æ‰¹å¤„ç†å¤§å°
  streaming_default: true  # é»˜è®¤æ˜¯å¦æµå¼è¾“å‡º
  optimize_first_token: true  # æ˜¯å¦ä¼˜åŒ–é¦–å­—å»¶è¿Ÿ
```

## å¯åŠ¨æœåŠ¡å™¨

### å¼€å‘ç¯å¢ƒ

```bash
python main.py
```

### ç”Ÿäº§ç¯å¢ƒ

ä½¿ç”¨Gunicornï¼ˆLinux/Macï¼‰ï¼š

```bash
gunicorn main:app -c gunicorn.conf.py
```

ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼š

```python
# gunicorn.conf.py
chdir = '/path/to/llm-api-server'
workers = 1  # å»ºè®®ä¸config.yamlä¸­çš„workersä¿æŒä¸€è‡´
threads = 2
worker_class = 'uvicorn.workers.UvicornWorker'
bind = '0.0.0.0:8050'
pidfile = 'gunicorn.pid'
accesslog = 'logs/gunicorn_access.log'
errorlog = 'logs/gunicorn_error.log'
loglevel = 'info'
```

## APIä½¿ç”¨ç¤ºä¾‹

æœåŠ¡å™¨æä¾›äº†ä¸OpenAIå…¼å®¹çš„APIæ¥å£ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•æ”¯æŒOpenAIçš„å®¢æˆ·ç«¯åº“è¿›è¡Œè°ƒç”¨ã€‚

### è®¤è¯

æ‰€æœ‰APIè¯·æ±‚éƒ½éœ€è¦åœ¨Headerä¸­åŒ…å«APIå¯†é’¥ï¼š

```
Authorization: Bearer sk-myapikey123456789
```

### èŠå¤©å®Œæˆï¼ˆChat Completionï¼‰

#### åŸºæœ¬èŠå¤©è¯·æ±‚

```bash
curl -X POST "http://localhost:8050/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-myapikey123456789" \
  -d '{
    "model": "DeepSeek-R1-0528-Qwen3-8B",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
      {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

#### æµå¼èŠå¤©è¯·æ±‚

```bash
curl -X POST "http://localhost:8050/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-myapikey123456789" \
  -d '{
    "model": "DeepSeek-R1-0528-Qwen3-8B",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
      {"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}
    ],
    "temperature": 0.7,
    "max_tokens": 500,
    "stream": true
  }'
```

### æ–‡æœ¬å®Œæˆï¼ˆText Completionï¼‰

```bash
curl -X POST "http://localhost:8050/v1/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-myapikey123456789" \
  -d '{
    "model": "DeepSeek-R1-0528-Qwen3-8B",
    "prompt": "è®²ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„ç¬‘è¯",
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### æ¨¡å‹åˆ—è¡¨

è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š

```bash
curl -X GET "http://localhost:8050/v1/models" \
  -H "Authorization: Bearer sk-myapikey123456789"
```

### åµŒå…¥ï¼ˆEmbeddingsï¼‰

ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ï¼š

```bash
curl -X POST "http://localhost:8050/v1/embeddings" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-myapikey123456789" \
  -d '{
    "model": "DeepSeek-R1-0528-Qwen3-8B",
    "input": "äººå·¥æ™ºèƒ½å°†å¦‚ä½•æ”¹å˜æˆ‘ä»¬çš„æœªæ¥ï¼Ÿ"
  }'
```

## ä½¿ç”¨Pythonå®¢æˆ·ç«¯

æ‚¨å¯ä»¥ä½¿ç”¨OpenAIçš„Pythonå®¢æˆ·ç«¯åº“ä¸æœåŠ¡å™¨è¿›è¡Œäº¤äº’ï¼š

```python
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-myapikey123456789",  # æ‚¨çš„APIå¯†é’¥
    base_url="http://localhost:8050/v1"  # æœåŠ¡å™¨åœ°å€
)

# èŠå¤©å®Œæˆ
response = client.chat.completions.create(
    model="DeepSeek-R1-0528-Qwen3-8B",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)

# æµå¼å“åº”
stream = client.chat.completions.create(
    model="DeepSeek-R1-0528-Qwen3-8B",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "å†™ä¸€ç¯‡çŸ­æ–‡ï¼Œæè¿°æœªæ¥çš„æ™ºèƒ½åŸå¸‚"}
    ],
    temperature=0.7,
    max_tokens=1000,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

## å¤šæ¨¡æ€æ¨¡å‹ç¤ºä¾‹

å¯¹äºæ”¯æŒå¤šæ¨¡æ€åŠŸèƒ½çš„æ¨¡å‹ï¼ˆå¦‚Qwen-VLã€LLaVAç­‰ï¼‰ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è°ƒç”¨ï¼š

### å¤šæ¨¡æ€èŠå¤©è¯·æ±‚

```bash
curl -X POST "http://localhost:8050/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-myapikey123456789" \
  -d '{
    "model": "Qwen-VL-Chat",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬ã€‚"},
      {"role": "user", "content": [
        {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEA..."}}  
      ]}
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### ä½¿ç”¨Pythonå®¢æˆ·ç«¯è¿›è¡Œå¤šæ¨¡æ€è¯·æ±‚

```python
from openai import OpenAI
import base64

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-myapikey123456789",
    base_url="http://localhost:8050/v1"
)

# è¯»å–å¹¶ç¼–ç å›¾åƒ
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# è·å–base64ç¼–ç çš„å›¾åƒ
image_path = "path/to/your/image.jpg"
base64_image = encode_image(image_path)

# åˆ›å»ºå¤šæ¨¡æ€è¯·æ±‚
response = client.chat.completions.create(
    model="Qwen-VL-Chat",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬ã€‚"},
        {"role": "user", "content": [
            {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡å¹¶åˆ†æå…¶ä¸­çš„ä¸»è¦å†…å®¹ã€‚"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ]}
    ],
    temperature=0.7,
    max_tokens=1000
)

print(response.choices[0].message.content)
```

## å¸¸è§é—®é¢˜è§£ç­”

### 1. å¦‚ä½•æ·»åŠ æ–°æ¨¡å‹ï¼Ÿ

åœ¨`config.yaml`çš„`models.available`éƒ¨åˆ†æ·»åŠ æ–°æ¨¡å‹çš„é…ç½®ï¼š

```yaml
models:
  available:
    # ç°æœ‰æ¨¡å‹...
    æ–°æ¨¡å‹åç§°:
      path: "/path/to/new/model"
      type: "æ¨¡å‹ç±»å‹"  # å¦‚llama, qwen, mistralç­‰
      max_tokens: 4096
      # å…¶ä»–é…ç½®...
```

### 2. å¦‚ä½•è§£å†³CUDAå†…å­˜ä¸è¶³çš„é—®é¢˜ï¼Ÿ

- å‡å°‘å·¥ä½œè¿›ç¨‹æ•°é‡ï¼ˆ`server.workers`ï¼‰
- é™ä½GPUå†…å­˜åˆ©ç”¨ç‡ï¼ˆ`models.available.æ¨¡å‹å.gpu_memory_utilization`ï¼‰
- ä½¿ç”¨æ¨¡å‹é‡åŒ–ï¼ˆ`models.available.æ¨¡å‹å.quantization`è®¾ç½®ä¸º"awq"æˆ–"gptq"ï¼‰
- å‡å°‘æœ€å¤§tokenæ•°ï¼ˆ`models.available.æ¨¡å‹å.max_tokens`ï¼‰

### 3. å¦‚ä½•æ·»åŠ æ–°çš„APIå¯†é’¥ï¼Ÿ

åœ¨`config.yaml`çš„`auth.api_keys`éƒ¨åˆ†æ·»åŠ æ–°çš„APIå¯†é’¥ï¼š

```yaml
auth:
  api_keys:
    # ç°æœ‰å¯†é’¥...
    - key: "sk-æ–°å¯†é’¥"
      name: "æ–°ç”¨æˆ·åç§°"
      permissions: ["chat", "completions"]  # æˆ–["all"]
```

### 4. æµå¼è¾“å‡ºæŠ¥é”™ "'LLM' object has no attribute 'generate_iterator'" æ€ä¹ˆè§£å†³ï¼Ÿ

è¿™ä¸ªé”™è¯¯é€šå¸¸æ˜¯å› ä¸ºä½¿ç”¨çš„vLLMç‰ˆæœ¬ä¸æ”¯æŒ`generate_iterator`æ–¹æ³•ã€‚è§£å†³æ–¹æ³•ï¼š

1. å‡çº§vLLMåˆ°æœ€æ–°ç‰ˆæœ¬ï¼š
```bash
pip install -U vllm
```

2. å¦‚æœä»ç„¶æœ‰é—®é¢˜ï¼Œå¯ä»¥ä¿®æ”¹`api/openai_compat.py`æ–‡ä»¶ä¸­çš„æµå¼ç”Ÿæˆå‡½æ•°ï¼Œä½¿ç”¨å¼‚æ­¥è¿­ä»£å™¨æ‰‹åŠ¨å®ç°æµå¼è¾“å‡ºã€‚

## æ€§èƒ½ä¼˜åŒ–

### GPUå†…å­˜ä¼˜åŒ–

- **å·¥ä½œè¿›ç¨‹æ•°é‡**ï¼šç¡®ä¿`config.yaml`ä¸­çš„`server.workers`ä¸`gunicorn.conf.py`ä¸­çš„`workers`ä¿æŒä¸€è‡´
- **å¼ é‡å¹¶è¡Œ**ï¼šå¯¹äºå¤§æ¨¡å‹ï¼Œå¯ä»¥å¢åŠ `tensor_parallel_size`ä»¥åœ¨å¤šä¸ªGPUä¸Šåˆ†å‰²æ¨¡å‹
- **é‡åŒ–**ï¼šä½¿ç”¨AWQæˆ–GPTQé‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨

### æ¨ç†é€Ÿåº¦ä¼˜åŒ–

- **æ‰¹å¤„ç†**ï¼šé€‚å½“å¢åŠ `max_batch_size`å¯ä»¥æé«˜ååé‡
- **é¦–å­—ä¼˜åŒ–**ï¼šä¿æŒ`optimize_first_token`ä¸º`true`ä»¥å‡å°‘é¦–å­—å»¶è¿Ÿ
- **ç¼“å­˜é¢„çƒ­**ï¼šä½¿ç”¨`preload`é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹

## æ•…éšœæ’é™¤

### æ¨¡å‹åŠ è½½å¤±è´¥

å¦‚æœé‡åˆ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š

1. ç¡®ä¿GPUå†…å­˜è¶³å¤Ÿï¼ˆä½¿ç”¨`nvidia-smi`æ£€æŸ¥ï¼‰
2. éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
4. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

### APIè¯·æ±‚é”™è¯¯

å¦‚æœAPIè¯·æ±‚è¿”å›é”™è¯¯ï¼š

1. ç¡®è®¤APIå¯†é’¥æ ¼å¼æ­£ç¡®ï¼ˆä»¥`sk-`å¼€å¤´ï¼‰
2. æ£€æŸ¥è¯·æ±‚æ ¼å¼æ˜¯å¦ç¬¦åˆOpenAI APIè§„èŒƒ
3. éªŒè¯è¯·æ±‚çš„æ¨¡å‹æ˜¯å¦åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­
4. æ£€æŸ¥æ˜¯å¦è¶…å‡ºäº†tokené™åˆ¶

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼è¯·é€šè¿‡GitHub Issuesæˆ–Pull Requestså‚ä¸é¡¹ç›®å¼€å‘ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚